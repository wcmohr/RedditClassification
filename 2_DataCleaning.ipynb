{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ea56c7-2158-44b4-a906-b10f8b4b1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/william/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#NLP Libs\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from cleantext import clean\n",
    "import xgboost as xgb\n",
    "\n",
    "# Lemmatizing Libs\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc70fe-61c5-4114-9ef1-0f21da239003",
   "metadata": {},
   "source": [
    "[how to get latest file](https://stackoverflow.com/questions/39327032/how-to-get-the-latest-file-in-a-folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688c439b-a8fb-48b6-b00d-ff5151e6d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = glob.glob('./scitech_data_scraped/*.csv') #\n",
    "latest_file = max(list_of_files, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb39fdd-c4cb-4a78-bf90-0b294b238198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (17,21,34,63,68,112) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "sci_tech_data = pd.read_csv(latest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ac9ae-2f7b-41ca-b36e-bad7570ad35c",
   "metadata": {},
   "source": [
    "Below I drop duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce600eb6-498d-4778-a693-33f8982b78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_tech_data.drop_duplicates(subset = ['selftext','title'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ccf35-36ec-4994-ac40-ac7fbfe04e10",
   "metadata": {},
   "source": [
    "Below I select the relevent rows that have tech or science as the subreddits and save the feature space, 'title' and the target, 'subreddit' to respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdf2939-2d72-465d-8f57-5bea28a0d704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r/technology              6692\n",
       "r/science                 3165\n",
       "u/MEGA-Technology          193\n",
       "u/Ok-Technology-1912       142\n",
       "u/Candid-Science-7189      132\n",
       "u/Gold-Science-2230         94\n",
       "u/Away-Technology-4883      63\n",
       "u/Any-Technology-9975       61\n",
       "u/Dazzling-Science-652      34\n",
       "u/AVID-Technology           30\n",
       "u/Content-Technology-7      19\n",
       "u/Bugd-Technology            7\n",
       "u/kretoss-technology         5\n",
       "u/SecureAge-Technology       4\n",
       "u/Thin-Science-6996          3\n",
       "u/OV-Technology              3\n",
       "u/Slow-Technology-9949       3\n",
       "u/Ok-Science-1826            2\n",
       "u/FS-Technology              2\n",
       "u/A-science-enthusiast       2\n",
       "u/Efficient-Science-80       1\n",
       "u/Ok-Science-9480            1\n",
       "u/No-Technology-2687         1\n",
       "u/Jaded-Technology-332       1\n",
       "u/more-technology-00         1\n",
       "u/Dear-Technology-6015       1\n",
       "u/Then-Technology-9558       1\n",
       "u/MR-Technology              1\n",
       "u/Fearless-Science-103       1\n",
       "u/Worth-Science-4441         1\n",
       "u/awesome-technology         1\n",
       "u/visimens-technology        1\n",
       "u/Fearless-Science-693       1\n",
       "u/TheSweet-Science           1\n",
       "u/Primary-Science-4968       1\n",
       "u/Glum-Science-6510          1\n",
       "u/Edu-Technology             1\n",
       "Name: subreddit_name_prefixed, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_tech_data['subreddit_name_prefixed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afe67e9-f4ff-4e02-aff8-0732016ec923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subreddit = sci_tech_data.loc[(sci_tech_data['subreddit'].isin(\n",
    "                                ['technology','science']), 'subreddit')]\n",
    "title = sci_tech_data.loc[(sci_tech_data['subreddit'].isin(\n",
    "                                ['technology','science']),'title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890906d5-fc0c-45b6-bcc9-0cd0f84beb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = title\n",
    "y = subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844a5dd4-2153-4c53-950e-9a2739fbc635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59127eba-783d-4f6d-ba35-c6a0063f5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.index, y_train.index, X_test.index, y_test.index) = \\\n",
    "(range(0, X_train.shape[0]),range(0, X_train.shape[0]), \\\n",
    " range(0, X_test.shape[0]),range(0, X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2660d309-13e8-4f33-8ab0-fcfc6824a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7392,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cd508-78ba-467a-928e-a10e9c94b40a",
   "metadata": {},
   "source": [
    "I'll get POS counts on the raw data.\n",
    "[dict to df](https://sparkbyexamples.com/python/pandas-convert-list-of-dictionaries-to-dataframe/#:~:text=The%20from_records()%20method%20is,dicts%20%2C%20or%20from%20another%20DataFrame.); \n",
    "[spacy POS tagging](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/); [spacy POS tagging #2](https://www.geeksforgeeks.org/python-pos-tagging-and-lemmatization-using-spacy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62bef864-9944-40bf-a99c-c25e0474eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15267657372422890137: 2, 10554686591937588953: 3, 15308085513773655218: 2, 17109001835818727656: 1, 1292078113972184607: 1, 74: 1, 164681854541413346: 1, 12646065887601541794: 1}\n",
      "74. POS : 1\n",
      "164681854541413346. RB  : 1\n",
      "1292078113972184607. IN  : 1\n",
      "10554686591937588953. JJ  : 3\n",
      "12646065887601541794. .   : 1\n",
      "15267657372422890137. DT  : 2\n",
      "15308085513773655218. NN  : 2\n",
      "17109001835818727656. VBD : 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n",
    "\n",
    "# Counting the frequencies of different fine-grained tags:\n",
    "TAG_counts = doc.count_by(spacy.attrs.TAG)\n",
    "\n",
    "print(TAG_counts)\n",
    "for k,v in sorted(TAG_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "648cbe04-35aa-4bde-8cb6-a195941ea50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "parts_of_speech = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e882e36-76a3-4791-9555-dc5391c3f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_stats(array):\n",
    "    parts_of_speech = []\n",
    "    for title in array:\n",
    "        tokenized = nlp(title)\n",
    "        pos_counts = {}\n",
    "        for token in tokenized:\n",
    "            if token.pos_ not in pos_counts.keys():\n",
    "                pos_counts[token.pos_] = 1\n",
    "            else :\n",
    "                pos_counts[token.pos_] += 1\n",
    "        parts_of_speech.append(pos_counts)\n",
    "    return pd.DataFrame(parts_of_speech).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3006a19e-57f0-4e99-a933-88eab7f91c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6788419913419913"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier()\n",
    "dc.fit(X_train,y_train)\n",
    "dc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f222fad-1820-401d-8a10-36973fe6229c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Proving',\n",
       " 'a',\n",
       " 'point:',\n",
       " '+|-',\n",
       " 'karma',\n",
       " 'has',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'post']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2921cc42-aa0b-4a28-bb40-98e684d8ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_stats(titles):\n",
    "    '''generate title statistics from a 1d object of title and return as a \n",
    "    DataFrame '''\n",
    "    # Char length, word count, max and avg word length\n",
    "    title_stats = pd.DataFrame(titles, columns = ['title'])\n",
    "    title_stats['title_length'] = [len(titles[i]) for i in range(len(titles))]\n",
    "    title_stats['title_word_count'] = [len(titles[i].split(' ')) for \\\n",
    "                                        i in range(len(titles))]\n",
    "    title_stats['max_word_length'] = [max(map(len, title.split(' '))) \\\n",
    "                         for title in titles]\n",
    "    title_stats['avg_word_length'] = title_stats['title_length']/title_stats['title_word_count']\n",
    "    \n",
    "    # POS stats\n",
    "    parts_of_speech = []\n",
    "    \n",
    "    for title in titles:\n",
    "        tokenized = nlp(title)\n",
    "        pos_counts = {}\n",
    "        for token in tokenized:\n",
    "            if token.pos_ not in pos_counts.keys():\n",
    "                pos_counts[token.pos_] = 1\n",
    "            else :\n",
    "                pos_counts[token.pos_] += 1\n",
    "        parts_of_speech.append(pos_counts)\n",
    "    title_stats = title_stats.join(pd.DataFrame(parts_of_speech).fillna(0))\n",
    "    return title_stats.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cf06d-db37-4089-bd96-f7d45106476b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e239a59-de19-4565-8838-604da49a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(titles):\n",
    "    '''tokenize a 1d object containing strings, returning a list of lists of \n",
    "    tokenized words'''\n",
    "    \n",
    "    tokenized = []\n",
    "    for title in titles:\n",
    "        tokenized.append(nlp(title))\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8387aa3d-2f77-4c97-a1c7-3f92a39cce08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokens[0][0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32dd7704-0a5a-4785-b719-76c2e04d8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(titles, tokenized):\n",
    "    'get the stats features that will be used in predicting'\n",
    "    \n",
    "    features = []\n",
    "    for title in titles:\n",
    "        tokenized = nlp(title)\n",
    "        for token in tokenized:\n",
    "            if token.pos_ not in features:\n",
    "                features.append(token.pos_)\n",
    "            if token.tag_ not in features:\n",
    "                features.append(token.tag_)       \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "962dab07-7c94-4edc-8a85-d0b8068570c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ff33a6-1d18-46e1-af22-0d84c00125d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = pos_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dc1d386-5fbd-421c-9a80-17d07d8c1e1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VERB',\n",
       " 'VBG',\n",
       " 'DET',\n",
       " 'DT',\n",
       " 'NOUN',\n",
       " 'NN',\n",
       " 'PUNCT',\n",
       " ':',\n",
       " 'X',\n",
       " 'LS',\n",
       " 'PROPN',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'PRON',\n",
       " 'PART',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'ADP',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'AUX',\n",
       " 'VBP',\n",
       " 'VBN',\n",
       " 'ADJ',\n",
       " 'JJ',\n",
       " '.',\n",
       " 'MD',\n",
       " 'PRP',\n",
       " 'SCONJ',\n",
       " 'RP',\n",
       " 'VBD',\n",
       " 'ADV',\n",
       " 'RB',\n",
       " 'JJR',\n",
       " 'SYM',\n",
       " '$',\n",
       " 'NUM',\n",
       " 'CD',\n",
       " 'POS',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'HYPH',\n",
       " '-LRB-',\n",
       " '-RRB-',\n",
       " '``',\n",
       " \"''\",\n",
       " 'CCONJ',\n",
       " 'CC',\n",
       " 'WRB',\n",
       " 'INTJ',\n",
       " 'UH',\n",
       " 'XX',\n",
       " 'WP',\n",
       " 'PRP$',\n",
       " 'JJS',\n",
       " 'NNPS',\n",
       " 'RBR',\n",
       " 'EX',\n",
       " 'RBS',\n",
       " 'PDT',\n",
       " 'FW',\n",
       " 'ADD',\n",
       " 'NFP',\n",
       " 'SPACE',\n",
       " '_SP',\n",
       " 'WP$',\n",
       " 'AFX']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adfd38b4-3db8-40e9-b88a-deaacfec4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feat_dict = dict(zip(pos_features,np.zeros(len(pos_features)).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d18adffe-7682-4d49-8267-0249ff5f417f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VERB': 0,\n",
       " 'VBG': 0,\n",
       " 'DET': 0,\n",
       " 'DT': 0,\n",
       " 'NOUN': 0,\n",
       " 'NN': 0,\n",
       " 'PUNCT': 0,\n",
       " ':': 0,\n",
       " 'X': 0,\n",
       " 'LS': 0,\n",
       " 'PROPN': 0,\n",
       " 'NNP': 0,\n",
       " 'VBZ': 0,\n",
       " 'PRON': 0,\n",
       " 'PART': 0,\n",
       " 'TO': 0,\n",
       " 'VB': 0,\n",
       " 'ADP': 0,\n",
       " 'IN': 0,\n",
       " 'NNS': 0,\n",
       " 'AUX': 0,\n",
       " 'VBP': 0,\n",
       " 'VBN': 0,\n",
       " 'ADJ': 0,\n",
       " 'JJ': 0,\n",
       " '.': 0,\n",
       " 'MD': 0,\n",
       " 'PRP': 0,\n",
       " 'SCONJ': 0,\n",
       " 'RP': 0,\n",
       " 'VBD': 0,\n",
       " 'ADV': 0,\n",
       " 'RB': 0,\n",
       " 'JJR': 0,\n",
       " 'SYM': 0,\n",
       " '$': 0,\n",
       " 'NUM': 0,\n",
       " 'CD': 0,\n",
       " 'POS': 0,\n",
       " ',': 0,\n",
       " 'WDT': 0,\n",
       " 'HYPH': 0,\n",
       " '-LRB-': 0,\n",
       " '-RRB-': 0,\n",
       " '``': 0,\n",
       " \"''\": 0,\n",
       " 'CCONJ': 0,\n",
       " 'CC': 0,\n",
       " 'WRB': 0,\n",
       " 'INTJ': 0,\n",
       " 'UH': 0,\n",
       " 'XX': 0,\n",
       " 'WP': 0,\n",
       " 'PRP$': 0,\n",
       " 'JJS': 0,\n",
       " 'NNPS': 0,\n",
       " 'RBR': 0,\n",
       " 'EX': 0,\n",
       " 'RBS': 0,\n",
       " 'PDT': 0,\n",
       " 'FW': 0,\n",
       " 'ADD': 0,\n",
       " 'NFP': 0,\n",
       " 'SPACE': 0,\n",
       " '_SP': 0,\n",
       " 'WP$': 0,\n",
       " 'AFX': 0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79949a6-f04b-4119-bfd7-62ecb80afe2f",
   "metadata": {},
   "source": [
    "Get tokens for X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05a3c2f5-e489-4ce1-99f3-11c47d0b279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd79229a-e98c-4368-84fc-50c7fe3cf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokens = tokenize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b0042707-1617-4d30-b690-a8fa2f89e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_stats(titles, tokenized, pos_feat_dict):\n",
    "    '''generate title statistics from a 1d object of title and return as a \n",
    "    DataFrame '''\n",
    "    # Char length, word count, max and avg word length\n",
    "    title_stats = pd.DataFrame()\n",
    "    title_stats['title_length'] = [len(titles[i]) for i in range(len(titles))]\n",
    "    title_stats['title_word_count'] = [len(titles[i].split(' ')) for \\\n",
    "                                        i in range(len(titles))]\n",
    "    title_stats['max_word_length'] = [max(map(len, title.split(' '))) \\\n",
    "                         for title in titles]\n",
    "    title_stats['avg_word_length'] = title_stats['title_length']/title_stats['title_word_count']\n",
    "    \n",
    "    # POS counts, fine pos tag counts    \n",
    "    parts_of_speech = []\n",
    "\n",
    "    for tokens in tokenized:\n",
    "\n",
    "        pos_counts = pos_feat_dict\n",
    "        for token in tokens:\n",
    "                pos_counts[token.pos_] += 1\n",
    "                pos_counts[token.tag_] += 1\n",
    "        parts_of_speech.append(pos_counts)\n",
    "    # combine \n",
    "    pos_df = pd.DataFrame(parts_of_speech).fillna(0)\n",
    "    tstats_pos = pd.concat([title_stats, pos_df],axis = 1)\n",
    "    return  tstats_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e2d14-762a-404a-9530-1c84ec05337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67353a4e-c994-4286-a7f0-350d4706317c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VERB': 14914,\n",
       " 'VBG': 2720,\n",
       " 'DET': 6897,\n",
       " 'DT': 7191,\n",
       " 'NOUN': 30151,\n",
       " 'NN': 21455,\n",
       " 'PUNCT': 12951,\n",
       " ':': 1451,\n",
       " 'X': 208,\n",
       " 'LS': 11,\n",
       " 'PROPN': 20461,\n",
       " 'NNP': 19382,\n",
       " 'VBZ': 4257,\n",
       " 'PRON': 5366,\n",
       " 'PART': 3395,\n",
       " 'TO': 2005,\n",
       " 'VB': 4637,\n",
       " 'ADP': 12893,\n",
       " 'IN': 13525,\n",
       " 'NNS': 8927,\n",
       " 'AUX': 5209,\n",
       " 'VBP': 2638,\n",
       " 'VBN': 2588,\n",
       " 'ADJ': 9974,\n",
       " 'JJ': 9055,\n",
       " '.': 3612,\n",
       " 'MD': 1446,\n",
       " 'PRP': 2545,\n",
       " 'SCONJ': 1724,\n",
       " 'RP': 383,\n",
       " 'VBD': 1799,\n",
       " 'ADV': 3302,\n",
       " 'RB': 3386,\n",
       " 'JJR': 571,\n",
       " 'SYM': 797,\n",
       " '$': 351,\n",
       " 'NUM': 3138,\n",
       " 'CD': 3138,\n",
       " 'POS': 943,\n",
       " ',': 3404,\n",
       " 'WDT': 523,\n",
       " 'HYPH': 1846,\n",
       " '-LRB-': 499,\n",
       " '-RRB-': 541,\n",
       " '``': 780,\n",
       " \"''\": 770,\n",
       " 'CCONJ': 3003,\n",
       " 'CC': 3003,\n",
       " 'WRB': 728,\n",
       " 'INTJ': 193,\n",
       " 'UH': 195,\n",
       " 'XX': 88,\n",
       " 'WP': 488,\n",
       " 'PRP$': 1148,\n",
       " 'JJS': 346,\n",
       " 'NNPS': 1083,\n",
       " 'RBR': 280,\n",
       " 'EX': 102,\n",
       " 'RBS': 89,\n",
       " 'PDT': 36,\n",
       " 'FW': 69,\n",
       " 'ADD': 40,\n",
       " 'NFP': 49,\n",
       " 'SPACE': 71,\n",
       " '_SP': 71,\n",
       " 'WP$': 3,\n",
       " 'AFX': 2}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_feat_dict  # why is pos_feat_dict mutating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ded96d98-accd-4c98-896c-aeb1694880ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VERB': 0,\n",
       " 'VBG': 0,\n",
       " 'DET': 0,\n",
       " 'DT': 0,\n",
       " 'NOUN': 0,\n",
       " 'NN': 0,\n",
       " 'PUNCT': 0,\n",
       " ':': 0,\n",
       " 'X': 0,\n",
       " 'LS': 0,\n",
       " 'PROPN': 0,\n",
       " 'NNP': 0,\n",
       " 'VBZ': 0,\n",
       " 'PRON': 0,\n",
       " 'PART': 0,\n",
       " 'TO': 0,\n",
       " 'VB': 0,\n",
       " 'ADP': 0,\n",
       " 'IN': 0,\n",
       " 'NNS': 0,\n",
       " 'AUX': 0,\n",
       " 'VBP': 0,\n",
       " 'VBN': 0,\n",
       " 'ADJ': 0,\n",
       " 'JJ': 0,\n",
       " '.': 0,\n",
       " 'MD': 0,\n",
       " 'PRP': 0,\n",
       " 'SCONJ': 0,\n",
       " 'RP': 0,\n",
       " 'VBD': 0,\n",
       " 'ADV': 0,\n",
       " 'RB': 0,\n",
       " 'JJR': 0,\n",
       " 'SYM': 0,\n",
       " '$': 0,\n",
       " 'NUM': 0,\n",
       " 'CD': 0,\n",
       " 'POS': 0,\n",
       " ',': 0,\n",
       " 'WDT': 0,\n",
       " 'HYPH': 0,\n",
       " '-LRB-': 0,\n",
       " '-RRB-': 0,\n",
       " '``': 0,\n",
       " \"''\": 0,\n",
       " 'CCONJ': 0,\n",
       " 'CC': 0,\n",
       " 'WRB': 0,\n",
       " 'INTJ': 0,\n",
       " 'UH': 0,\n",
       " 'XX': 0,\n",
       " 'WP': 0,\n",
       " 'PRP$': 0,\n",
       " 'JJS': 0,\n",
       " 'NNPS': 0,\n",
       " 'RBR': 0,\n",
       " 'EX': 0,\n",
       " 'RBS': 0,\n",
       " 'PDT': 0,\n",
       " 'FW': 0,\n",
       " 'ADD': 0,\n",
       " 'NFP': 0,\n",
       " 'SPACE': 0,\n",
       " '_SP': 0,\n",
       " 'WP$': 0,\n",
       " 'AFX': 0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b83c30c-8cc8-40f9-a2b1-1f056c1a9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stats = title_stats(X_train, X_train_tokens, pos_feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2807aff-e26c-4e52-8542-22b9a844e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stats = title_stats(X_test, X_test_tokens, pos_feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9673b1ac-cd80-4103-ac6d-f1bdf442cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab153b68-bbf9-4b53-b8ed-8b69b63d6c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jl/p0b83vx55ls1hskdgz1sddkw0000gn/T/ipykernel_1576/4191890788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 )\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_stats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c18e1de-8cf9-404a-8cf0-2e54f6f75438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7392, 71)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8303852b-5dac-4e20-9715-8d681fb3e0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2465, 71)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a7fd7f3-347d-4fa4-9593-9e0495dbc5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2465, 22)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fine_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdbc8f73-acb8-4084-aa78-a4a13f76c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_length</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>max_word_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>VERB</th>\n",
       "      <th>VBG</th>\n",
       "      <th>DET</th>\n",
       "      <th>DT</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NN</th>\n",
       "      <th>...</th>\n",
       "      <th>EX</th>\n",
       "      <th>RBS</th>\n",
       "      <th>PDT</th>\n",
       "      <th>FW</th>\n",
       "      <th>ADD</th>\n",
       "      <th>NFP</th>\n",
       "      <th>SPACE</th>\n",
       "      <th>_SP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>AFX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>5.884615</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6.090909</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>291</td>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>6.767442</td>\n",
       "      <td>14914</td>\n",
       "      <td>2720</td>\n",
       "      <td>6897</td>\n",
       "      <td>7191</td>\n",
       "      <td>30151</td>\n",
       "      <td>21455</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7392 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      title_length  title_word_count  max_word_length  avg_word_length   VERB  \\\n",
       "0               65                13                7         5.000000  14914   \n",
       "1               70                10               11         7.000000  14914   \n",
       "2               95                20                8         4.750000  14914   \n",
       "3              153                26               10         5.884615  14914   \n",
       "4               67                11                8         6.090909  14914   \n",
       "...            ...               ...              ...              ...    ...   \n",
       "7387            85                10               12         8.500000  14914   \n",
       "7388            44                 6               17         7.333333  14914   \n",
       "7389            68                10               10         6.800000  14914   \n",
       "7390            42                 6                8         7.000000  14914   \n",
       "7391           291                43               13         6.767442  14914   \n",
       "\n",
       "       VBG   DET    DT   NOUN     NN  ...   EX  RBS  PDT  FW  ADD  NFP  SPACE  \\\n",
       "0     2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "1     2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "2     2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "3     2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "4     2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "...    ...   ...   ...    ...    ...  ...  ...  ...  ...  ..  ...  ...    ...   \n",
       "7387  2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "7388  2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "7389  2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "7390  2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "7391  2720  6897  7191  30151  21455  ...  102   89   36  69   40   49     71   \n",
       "\n",
       "      _SP  WP$  AFX  \n",
       "0      71    3    2  \n",
       "1      71    3    2  \n",
       "2      71    3    2  \n",
       "3      71    3    2  \n",
       "4      71    3    2  \n",
       "...   ...  ...  ...  \n",
       "7387   71    3    2  \n",
       "7388   71    3    2  \n",
       "7389   71    3    2  \n",
       "7390   71    3    2  \n",
       "7391   71    3    2  \n",
       "\n",
       "[7392 rows x 71 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "64342b1f-f9ff-4ccd-b3ad-21c4c234e8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924949290060852"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(X_test_stats, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdef7c-bb23-4969-9949-cb547e71eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_fine_stats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a388c-5fab-494a-9680-c511ef534bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_train_fine_stats.columns) == set(X_test_fine_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f13cae-d13a-4c46-9ff2-21e1571a35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_fine_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed265d2-e07d-4ef7-acb6-1cd55e6c4937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08baee-b0bd-41f5-82fb-70e73056d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n",
    "\n",
    "# Counting the frequencies of different fine-grained tags:\n",
    "TAG_counts = doc.count_by(spacy.attrs.TAG)\n",
    "\n",
    "print(TAG_counts)\n",
    "for k,v in sorted(TAG_counts.items()):\n",
    "    print(f'{doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3463d-532e-416c-a212-4e8cfb5c34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_counts = .count_by(spacy.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29535815-1db4-4998-ae55-6ef89668d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = title_stats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255742d1-b4a4-421a-9738-d54b716ce9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = title_stats(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3205c-8b1d-4a9b-bc04-35a44e8b76de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711257b-2318-418c-abfb-ecbe411c3fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abda165-6025-4aef-978b-6dab007a63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a4481-5f56-491c-b041-93688d1b0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec47ea3-1a2e-4413-a90d-329271c73473",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(stats[['title_length', 'title_word_count', 'max_word_length',\n",
    "       'avg_word_length', 'VERB', 'DET', 'NOUN', 'PUNCT', 'X', 'PROPN', 'PRON',\n",
    "       'PART', 'ADP', 'AUX', 'ADJ', 'SCONJ', 'ADV', 'SYM', 'NUM', 'CCONJ',\n",
    "       'INTJ', 'SPACE']],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5a7a0-5c66-4d4f-b3bd-05004b3e263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0891a-7b7d-4806-ac81-cb217b9d8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(test_stats[['title_length', 'title_word_count', 'max_word_length',\n",
    "       'avg_word_length', 'VERB', 'DET', 'NOUN', 'PUNCT', 'X', 'PROPN', 'PRON',\n",
    "       'PART', 'ADP', 'AUX', 'ADJ', 'SCONJ', 'ADV', 'SYM', 'NUM', 'CCONJ',\n",
    "       'INTJ', 'SPACE']],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682c7a6-617b-4364-8670-e21cad0074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stats.columns) == set(test_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b1a83-d880-4931-9f31-d4c348599432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cla import Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174d223-de6d-44d2-b08b-85ea374f948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(stats,y_train), rfc.score(test_stats,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e9b3e-3cce-4cef-9b41-05db9012d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech_stats=pos_stats(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24c111-1799-4d10-a2ae-e1c07c5060f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech_stats.join(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e0089-783e-4907-a18c-90663395f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts =title_stats(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587f2e5-976c-4cfc-bb1f-1d3ea90e0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos = pos_stats(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29a2be-1443-45d9-9e3e-0a46dfbb26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pos = pos_stats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713e524-3236-4527-9ea0-5b84093923f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae572653-26cf-493d-889a-0bda91b41151",
   "metadata": {},
   "source": [
    "[source: strip characters from string in series](https://stackoverflow.com/questions/13682044/remove-unwanted-parts-from-strings-in-a-column)\n",
    "[source: remove punctuation](https://www.google.com/search?q=how+to+replace+punctuation+with+regular+expression+python&rlz=1C5CHFA_enUS983US983&oq=how+to+replace+punctuation+with+regular&aqs=chrome.1.69i57j33i160l2.10574j0j7&sourceid=chrome&ie=UTF-8#kpvalbx=_7SabY4OANaSs0PEP042roAM_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db08b99-4e52-49cc-9d72-21b8d80a0d17",
   "metadata": {},
   "source": [
    "Get rid of puctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66839975-dd0a-4040-a509-3bef28d72334",
   "metadata": {},
   "source": [
    "[remove stop words](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604a817-dff0-4876-872c-82092263fc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57824bc-b44f-4287-8a48-b314aa1ce2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered(data):\n",
    "    # replace \"-\" w/ 'hyphen', then remove emojis, punctuation, digits, and urls\n",
    "    # while keeping an indicator or url usage.\n",
    "    data_filtered = data.map(lambda x: clean(\n",
    "                            re.sub('-',repl=' hyphen ', string = x) , no_emoji=True,\n",
    "                                no_punct=True,no_digits=True, no_urls=True))\n",
    "\n",
    "    # remove leftovers from the 'clean' function  \n",
    "\n",
    "    data_filtered = data_filtered.map(lambda x: re.sub('(0|\\|)',\n",
    "                                                    repl='',string = x))\n",
    "    #replacing hyphen with '-'\n",
    "    data_filtered = data_filtered.map(lambda x: re.sub('hyphen',repl='-', string = x))\n",
    "    # remove stop words   \n",
    "    data_filtered = data_filtered.map(lambda x: ' '.join([word for word \n",
    "                                              in x.split() if word not in \n",
    "                                              stop_words]))\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f57f34-bb61-408e-b60d-1075d5900e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filtered = filtered(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b5818-3b48-47b4-a428-2b0e67ad64e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace \"-\" w/ 'hyphen', then remove emojis, punctuation, digits, and urls\n",
    "# while keeping an indicator or url usage.\n",
    "\n",
    "X_train_filtered = X_train.map(lambda x: clean(\n",
    "                            re.sub('-',repl=' hyphen ', string = x) , no_emoji=True,\n",
    "                                no_punct=True,no_digits=True, no_urls=True))\n",
    "\n",
    "# remove leftovers from the 'clean' function  \n",
    "\n",
    "X_train_filtered = X_train_filtered.map(lambda x: re.sub('(0|\\|)',\n",
    "                                                    repl='',string = x))\n",
    "#replacing hyphen with '-'\n",
    "X_train_filtered = X_train_filtered.map(lambda x: re.sub('hyphen',repl='-', string = x))\n",
    "# remove stop words   \n",
    "X_train_filtered = X_train_filtered.map(lambda x: ' '.join([word for word \n",
    "                                          in x.split() if word not in \n",
    "                                          stop_words]))\n",
    "\n",
    "print((sum([len(s) for s in X_train])-sum([len(s) for s in X_train_filtered]))\n",
    "    /sum([len(s) for s in X_train])\n",
    ")\n",
    "\n",
    "# I'll keep (<url>) group in the words, in case there is differential frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a0cbc-c601-485e-a82c-c47fb5531484",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **Lemmatization** -- [sources](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179e519-961b-454e-b7e8-4a2d4aae7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218a8a5-05f8-4815-abf0-9790941b5555",
   "metadata": {},
   "source": [
    "[Lemmatizing w/ POS](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6514145-4e90-480c-bf69-a7656d9834ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2616f-defc-49f2-8a9b-2b5f1c1b6b39",
   "metadata": {},
   "source": [
    "To-do/idea: get POS counts [source](https://stackoverflow.com/questions/20960777/python-how-to-count-pos-tags-from-from-a-sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89443d51-5586-451a-a480-b8cd235bfebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "X_train_lemmatized = []\n",
    "for title in X_train_filtered:\n",
    "    X_train_lemmatized.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for \\\n",
    "                               w in nltk.word_tokenize(title)]))\n",
    "X_train_lemmatized;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23490225-0c9f-4c90-8117-9b634da4f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemmatized = pd.Series(data = X_train_lemmatized, index = X_train_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08167d63-0b92-4480-b7cd-c755718b3e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_lemmatized = []\n",
    "for title in X_test_filtered:\n",
    "    X_test_lemmatized.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for \\\n",
    "                               w in nltk.word_tokenize(title)]))\n",
    "X_test_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a0b13-1d98-4e97-b324-77d5d04fbf95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a3422-2f13-4e0e-8502-f52042f01889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to DataFrame\n",
    "df = pd.DataFrame(X_train_lemmatized, columns = ['title']).join(pd.DataFrame(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df9357-4098-48ca-a434-0f6b0df3d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed19be-f89c-4410-96b0-ed432e2e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_lemmatized, columns = ['title']).join(pd.DataFrame(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497e187-8c16-4dc0-95d9-aa7cf27e3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ed03-8893-4189-9699-43f49dc67649",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### With words lemmatized and english stop words removed I will proceed with EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4c2b3-fd10-4d67-92cd-6ebf4cc6e442",
   "metadata": {},
   "source": [
    "Below I will inspect various distributions of title statistics after adding the statistics to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412d977-d0e5-4dda-83eb-d47b9fa08b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['title_length'] = [len(df.loc[i,'title']) for i in range(len(df['title']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357d8c6-8480-41d8-8d57-93619f12f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_word_count'] = [len(df.loc[i,'title'].split(' ')) for i in range(len(df['title']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020abad-5b48-4808-9ef5-a39e9ad72dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_word_length'] = [max(map(len, title.split(' '))) \\\n",
    "                         for title in df['title']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4cff3-0ea8-4c6e-83e0-ab9fe1081e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_length'] = df['title_length']/df['title_word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786928bc-fcf4-477b-a581-b3e5657558db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit_indicator']=[1 if sub == 'technology' else 0 for sub in df['subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b9a14-684f-4625-9517-170464c8dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe945764-9d92-4ab2-b564-bc2314a40752",
   "metadata": {},
   "source": [
    "###### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9de48-b142-47bb-87e5-820e11fa642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df, x = 'title_word_count', hue = 'subreddit', stat='density', common_norm = False, bins = 45).set(title = 'Distribution of title word counts by subreddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447637c2-20a2-4ce3-98db-5a2919868f32",
   "metadata": {},
   "source": [
    "Above we can see that shorter titles have a better chance of having been posted to technology, while longer titles are more likely from science even after accounting for baseline frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac4d71-7db1-4cb4-90d1-2fdef8a44d48",
   "metadata": {},
   "source": [
    "Above we see that there are many posts between 0 and 20 words long with a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373fb51-2c1c-4290-91fb-c70555e8a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df, x = 'title_length', hue = 'subreddit', stat='density', common_norm = False).set(title = 'Distribution of title character length by subreddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836a7b2-5af2-43e4-9068-21ce86d5e1c3",
   "metadata": {},
   "source": [
    "Above we see that generally title character lengths under around 80 are more likely to have been from technology, and above 80 from science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a490f-d777-429d-89ad-f3cf3d3bfce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df, x = 'avg_word_length', hue = 'subreddit', \n",
    "             stat='density', bins = 100, common_norm = False).set(\n",
    "    title = 'Distribution of title word counts by subreddit', xlim = [4,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca38b8-8056-411a-9bd1-266b08e80f9b",
   "metadata": {},
   "source": [
    "[adjusting bins](https://stackoverflow.com/questions/48990594/how-to-draw-distribution-plot-for-discrete-variables-in-seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d56e6-3d23-4715-8f37-922069b4b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = df, x = 'max_word_length', hue = 'subreddit', \n",
    "             stat='density', common_norm = False, bins=np.arange(0,21)).set(\n",
    "    title = 'Distribution of title word counts by subreddit', xlim = [0,20], \n",
    "    xticks = range(0,21));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd8a8b-ff98-4d9d-a21f-a550cf4552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c385bd3-190c-4b8c-a7b2-a1ffe31b5c6d",
   "metadata": {},
   "source": [
    "Above we see that word length, character count, and word counts all have some correlation with the particular subredit, though the correlation is not strong.  However, from the density plots and the clear separation in likelihoods it is apparent that valuable information would likely be picked up from a tree-based classification model.  It is promising that the title statistics gathered thus far are not fully correlated with each other as this means they can provide non-redundant information to the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc012395-1921-4bfa-a23d-11be292bb07b",
   "metadata": {},
   "source": [
    "I'll next look at word count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b4d65-7090-4e45-9dab-cd14f9817ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vectors = cv.fit_transform(df[df['subreddit']=='technology']['title'])\n",
    "vectors.A\n",
    "wc_vec = pd.DataFrame(vectors.A, columns = cv.get_feature_names_out())\n",
    "wc_vec.sum().sort_values(ascending = False)[0:15].plot(kind = 'bar')\\\n",
    ".set(title = '15 most common words word count -- technology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79fd7b-647e-4e4b-9f6e-2b1426a96c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vectors = cv.fit_transform(df[df['subreddit']=='science']['title'])\n",
    "vectors.A\n",
    "wc_vec = pd.DataFrame(vectors.A, columns = cv.get_feature_names_out())\n",
    "wc_vec.sum().sort_values(ascending = False)[0:15].plot(kind = 'bar')\\\n",
    ".set(title = '15 most common words word count -- science')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185bd6d-4c61-45c7-9cf3-8baa5ec86349",
   "metadata": {},
   "source": [
    "Above we see that there is not much overlap within the 15 most common words.  This indicates that there is a good chance that these common words will help with distinguishing between the subreddits.  Of note is that proper nouns seem to feature heavily in technology and almost not at all in science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d0e51-85f6-4d78-ba8b-01245080091c",
   "metadata": {},
   "source": [
    "##### **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eb749-c4d0-47d7-82bf-cf2a41af3649",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c1c4e-b501-4ab9-b55d-5e16b0df6649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cbd3c6b-32a4-4a7b-a334-5aa8a463b0e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Modeling on title statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe75e9-fefe-4fe3-8f7a-b7bd5329a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_stats(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5aa24-1907-42a3-805c-e96b2a02ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a19af-7f62-4714-a716-f2e8f7509c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(stop_words='english', max_features=1_000,\n",
    "                             ngram_range=(1,2))\n",
    "logreg = LogisticRegression(penalty='elasticnet', max_iter=10_000, \\\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eed92c-3ef1-453e-97b0-a34b4d15550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lr_params = {\"logreg__C\": [.1,1,10],\n",
    "                    \"logreg__l1_ratio\": [.1,.5,.9],\n",
    "                   'tfidf_vec__max_df': [.9,.95,1.0],\n",
    "                   'tfidf_vec__min_df': [.001,.003]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b4017-99fe-4e29-a162-4e964e3fafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_lr_pipe = Pipeline([\n",
    "    ('tfidf_vec', tfidf_vec),\n",
    "    ('logreg', LogisticRegression(penalty='none', max_iter=10_000, random_state=33))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339779c-8b52-4de7-be6e-6c2aa940666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c7e2b-4641-49fc-b0d2-de767ec16e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_lr = GridSearchCV(tfidf_lr_pipe,tfidf_lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e1729-562f-4035-b6fb-cabea9d47c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198e581-7562-4db0-8cb8-5351cff84a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf23c0f-d2fe-469d-8ea5-346c15090a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " gs_tfidf_lr.fit(df['title'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfbdbc-ee4a-466f-850a-1993e6b4cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_lr.score(df['title'],y_train),gs_tfidf_lr.score(df['title'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638744e9-b422-4545-a8cc-618fc4f66c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_lr.score(X_test_lemmatized,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899943ca-b704-48f0-a271-e42a812c7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_xgb_pipe = Pipeline([\n",
    "    ('tfidf_vec', tfidf_vec),\n",
    "    ('xgb', xgb.XGBClassifier())])\n",
    "tfidf_xgb_params = {\"xgb__eta\": [.01,.03],\n",
    "                    'xgb__booster': ['gbtree','gblinear'],\n",
    "                   'tfidf_vec__max_df': [.9,.95,1.0],\n",
    "                   'tfidf_vec__min_df': [.001,.003]}\n",
    "gs_tfidf_xgb = GridSearchCV(tfidf_xgb_pipe,tfidf_xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6974c05-68be-41bb-a5b5-c6518f9812eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_xgb = [1 if sub == 'technology' else 0 for sub in y_train]\n",
    "y_test_xgb = [1 if sub == 'technology' else 0 for sub in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf38217-16cc-42f1-8184-66e6fd519387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_tfidf_xgb.fit(X_train_lemmatized,y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f2df4-b070-41d4-b952-e54144905d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb.score(X_train_lemmatized,y_train_xgb),gs_tfidf_xgb.score(X_test_lemmatized,y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe6b7a-70e7-4011-adfb-002d44b1e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb = GridSearchCV(tfidf_xgb_pipe,{'tfidf_vec__max_df': [0.9],\n",
    " 'tfidf_vec__min_df': [0.001],\n",
    " 'xgb__booster': ['gblinear'],\n",
    " 'xgb__eta': [0.01]})\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2488e5-7790-4ba5-a312-346ef75d74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_tfidf_xgb.fit(X_train_lemmatized,y_train_xgb)\n",
    "gs_tfidf_xgb.score(X_train_lemmatized,y_train_xgb),gs_tfidf_xgb.score(X_test_lemmatized,y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4d542-2ab3-4714-b899-885315b2d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee304b-b760-4f17-89d0-0879be42afdd",
   "metadata": {},
   "source": [
    "param choices:\n",
    "tfidf_xgb_params = {\"xgb__eta\": [.01,.03],\n",
    "                    'xgb__booster': ['gbtree','gblinear'],\n",
    "                   'tfidf_vec__max_df': [.9,.95,1.0],\n",
    "                   'tfidf_vec__min_df': [.001,.003]}\n",
    "score: \n",
    "\n",
    "(0.9143668831168831, 0.8892494929006085)\n",
    "\n",
    "gs_tfidf_xgb.best_params_ :\n",
    "\n",
    "{'tfidf_vec__max_df': 0.9,\n",
    " 'tfidf_vec__min_df': 0.001,\n",
    " 'xgb__booster': 'gblinear',\n",
    " 'xgb__eta': 0.01}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91353e-fd36-42e9-bfb5-74b1dd12c877",
   "metadata": {},
   "source": [
    "So far xgboost has provided the best prediction on the test set.  I will try to narrow in on the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade39f72-375b-42d1-b1f3-120e94649f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_xgb_pipe = Pipeline([\n",
    "    ('tfidf_vec', tfidf_vec),\n",
    "    ('xgb', xgb.XGBClassifier(seed = 1))])\n",
    "tfidf_xgb_params = {\"xgb__eta\": [.01,.015,.05],\n",
    "                    'xgb__booster': ['gbtree','gblinear'],\n",
    "                    # 'xgb__lambda': [.1,1,10],\n",
    "                    # 'xgb__alpha': [0,.1,1],\n",
    "                   'tfidf_vec__max_df': [.85,.9,.925],\n",
    "                   'tfidf_vec__min_df': [.0005,.001,.0015],}\n",
    "gs_tfidf_xgb = GridSearchCV(tfidf_xgb_pipe,tfidf_xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de890259-c73b-4add-aec0-e8a42e2636e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_tfidf_xgb.fit(X_train_lemmatized,y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ba3d1-e39d-48e2-b048-68cec3adda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e497a-6a44-496a-8798-5f7a6350eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb.best_params_\n",
    "{'tfidf_vec__max_df': 0.85,\n",
    " 'tfidf_vec__min_df': 0.001,\n",
    " 'xgb__alpha': 0,\n",
    " 'xgb__booster': 'gbtree',\n",
    " 'xgb__eta': 0.05,\n",
    " 'xgb__lambda': 0.1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5e61c-7482-4222-adef-2d7824c0871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb.score(X_train_lemmatized,y_train_xgb),gs_tfidf_xgb.score(X_test_lemmatized,y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40d2e2-a171-4c4b-99eb-3a887456e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_tfidf_xgb.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ac1ad-3da6-4486-9e50-0992916ee181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14344a0-2933-4691-818c-79f5ddf09691",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf_xgb = GridSearchCV(tfidf_xgb_pipe,{'tfidf_vec__max_df': [0.9],\n",
    " 'tfidf_vec__min_df': [0.001],\n",
    " 'xgb__booster': ['gblinear'],\n",
    " 'xgb__eta': [0.005]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e72e58-178b-47c3-9533-423e58657793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_tfidf_xgb.fit(X_train_lemmatized,y_train_xgb)\n",
    "gs_tfidf_xgb.score(X_train_lemmatized,y_train_xgb),gs_tfidf_xgb.score(X_test_lemmatized,y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08933737-c040-4c1a-9b0d-5e66ba26ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb5485-ee14-489c-a4bb-42ad12b23337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b5b91-33dd-4ede-a132-62ad52866d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15e6b1-f5fe-4e70-abe3-475e0ee1e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pos = xgb.XGBClassifier(seed = 1)\n",
    "xgb_pos.fit(X_train_pos.a,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a3812-4da3-4419-aeb9-50b853136e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
